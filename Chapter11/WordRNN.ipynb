{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "WordRNN.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMgf15HNHREDSBB3v/kiXGx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JangHanjun/Natural_Language_Programming/blob/main/Chapter11/WordRNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6RaqtKAHSkL0"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"Repeat is the best medicine for memory\".split()"
      ],
      "metadata": {
        "id": "mdGJpIg9TJWz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = list(set(sentence))\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1a34a7cTRF4",
        "outputId": "8ea2620c-5d12-4300-e119-072307ec82de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['memory', 'Repeat', 'the', 'for', 'best', 'medicine', 'is']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "word2index = {tkn : i for i, tkn in  enumerate(vocab, 1)}\n",
        "word2index['<unk>'] = 0\n",
        "print(word2index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2mGhgjzNTwu1",
        "outputId": "989070a7-5592-4266-8b8d-9ee1f9f53962"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'memory': 1, 'Repeat': 2, 'the': 3, 'for': 4, 'best': 5, 'medicine': 6, 'is': 7, '<unk>': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "idx2word = {tkn : i for i, tkn in word2index.items()}\n",
        "print(idx2word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2D3Y49mdUDDq",
        "outputId": "3a38b124-9417-400d-f80b-e1afdd04145e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{1: 'memory', 2: 'Repeat', 3: 'the', 4: 'for', 5: 'best', 6: 'medicine', 7: 'is', 0: '<unk>'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_data(sentence, word2index):\n",
        "  encoded = [word2index[token] for token in sentence] # 각 문자를 정수로\n",
        "  input_seq, label_seq = encoded[:-1], encoded[1:] # 입력, 레이블 시퀀스 분리\n",
        "\n",
        "  input_seq = torch.LongTensor(input_seq).unsqueeze(0)\n",
        "  label_seq = torch.LongTensor(label_seq).unsqueeze(0)\n",
        "\n",
        "  return input_seq, label_seq"
      ],
      "metadata": {
        "id": "rZLycjd6Ux3-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, Y = build_data(sentence, word2index)\n",
        "print(X)\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBL6WZ0CVggA",
        "outputId": "af668fc7-dd1e-4db9-e42c-9a3d5de928e5"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[2, 7, 3, 5, 6, 4]])\n",
            "tensor([[7, 3, 5, 6, 4, 1]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Net(nn.Module):\n",
        "  def __init__(self, vocab_size, input_size, hidden_size, batch_first=True):\n",
        "    super(Net, self).__init__()\n",
        "    self.embedding_layer = nn.Embedding(num_embeddings=vocab_size,\n",
        "                                        embedding_dim=input_size)\n",
        "    self.rnn_layer = nn.RNN(input_size, hidden_size, \n",
        "                            batch_first=batch_first)\n",
        "    self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    # 1 - 임베딩 층\n",
        "    # 크기변화 : (배치 크기, 시퀸스 길이) => (배치 크기, 시퀸스 길이, 임베딩 차원)\n",
        "    output = self.embedding_layer(x)\n",
        "\n",
        "    # 2 - RNN층\n",
        "    # 크기변화 : (배치 크기, 시퀸스 길이, 임베딩 차원)\n",
        "    # => output : (배치 크기, 시퀸스 길이, 은닉층 차원)\n",
        "    # hidden : (1, 배치 크기, 은닉층 크기)\n",
        "    output, hidden = self.rnn_layer(output)\n",
        "\n",
        "    # 3 - 최종 출력층\n",
        "    # 크기 변화 : (배치 크기, 시퀀스 길이, 은닉층 크기) => (배치 크기, 시퀀스 길이, 단어장 크기)\n",
        "    output = self.linear(output)\n",
        "\n",
        "    # 4 - view를 통해서 배치 차원 제거\n",
        "    # 크기 변화 : (배치 크기, 시퀀스 길이, 단어장 크기) => (배치 크기,*시퀸스 길이, 단어장 크기)\n",
        "    return output.view(-1, output.size(2))"
      ],
      "metadata": {
        "id": "1I8xdxaYWcIl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 하이퍼 파라미터\n",
        "vocab_size = len(word2index)\n",
        "input_size = 5    # 임베딩 된 차원의 크기 및 RNN 층 입력 차원의 크기\n",
        "hidden_size = 20  # RNN의 은닉층 크기"
      ],
      "metadata": {
        "id": "epBm4DBdZQQT"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Net(vocab_size, input_size, hidden_size, batch_first=True)\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params=model.parameters())"
      ],
      "metadata": {
        "id": "hXQgYodIZhIT"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = model(X)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KOyADtAaaS0j",
        "outputId": "68ef67a3-447d-43e0-c002-e4b8abc3e053"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.1653, -0.1343,  0.3411, -0.1654, -0.0481,  0.2472, -0.0657, -0.3647],\n",
            "        [ 0.2336,  0.5395,  0.5128, -0.1267, -0.2807,  0.1619, -0.1223, -0.5432],\n",
            "        [-0.0473, -0.7080, -0.1636,  0.0229, -0.2342,  0.3216,  0.0451, -0.0826],\n",
            "        [ 0.1025, -0.4546, -0.0592, -0.0205, -0.0484,  0.1557, -0.0595, -0.3603],\n",
            "        [ 0.0747, -0.2633, -0.1121, -0.1524,  0.0469,  0.1235, -0.0709, -0.2964],\n",
            "        [ 0.1439,  0.1084,  0.4935,  0.0635, -0.5253,  0.1524, -0.5706, -0.2625]],\n",
            "       grad_fn=<ViewBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(output.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hF9jPWrWaj98",
        "outputId": "c4dce4ee-981d-4ca2-b769-095d3ffc14ac"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([6, 8])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decode = lambda y : [idx2word.get(x) for x in y]"
      ],
      "metadata": {
        "id": "Pn4CaH7ra7S3"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for step in range(201):\n",
        "  # 경사 초기화\n",
        "  optimizer.zero_grad()\n",
        "  # 순방향 전파\n",
        "  output = model(X)\n",
        "  # 손실값 계산\n",
        "  loss = loss_function(output, Y.view(-1))\n",
        "  # 역방향 전파\n",
        "  loss.backward()\n",
        "  #매개변수 업데이트\n",
        "  optimizer.step()\n",
        "\n",
        "  if step % 40 == 0 :\n",
        "    print(\"[{:02d}/201] {:.4f} \".format(step+1, loss))\n",
        "    pred = output.softmax(-1).argmax(-1).tolist()\n",
        "    print(\" \".join([\"Repeat\"] + decode(pred)))\n",
        "    print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPT1QgqXbdZ0",
        "outputId": "461600e6-f5de-4181-bd2c-bf2704c9e07f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[01/201] 0.1499 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[41/201] 0.0977 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[81/201] 0.0699 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[121/201] 0.0533 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[161/201] 0.0424 \n",
            "Repeat is the best medicine for memory\n",
            "\n",
            "[201/201] 0.0347 \n",
            "Repeat is the best medicine for memory\n",
            "\n"
          ]
        }
      ]
    }
  ]
}